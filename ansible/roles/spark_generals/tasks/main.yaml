- name: Update
  apt:
    update_cache: yes
    upgrade: yes

- name: Install sudo, unzip, curl, vim and git
  package:
    name: "{{ item }}"
    state: present
  loop:
    - unzip
    - curl
    - wget
    - net-tools
    - git
    - vim
    - tree
    - lsof
    - netcat-traditional
    - telnet
    - iptables
    - iptables-persistent
    - default-jdk
    - scala

- name: Create an iptable config file
  ansible.builtin.template:
    src: iptables.rules
    dest: "/etc/iptables.rules"
    # mode: "0755"

- name: Get current VM subnet
  shell: |
    private_ip=$(hostname -I | awk '{print $1}')

    if [ -n "`echo $private_ip | grep -e '10.0.2'`" ]; then 
      echo "2"
    else
      echo "3"
    fi
  args: 
    executable:
      /bin/bash
  register: subnet

- name: Change the subnet in the iptable config file
  ansible.builtin.lineinfile:
    path: "/etc/iptables.rules"
    regexp: '^-4 -A INPUT -s 10.0.CHANGE.0/24 ! -i lo -j ACCEPT'
    line: "-4 -A INPUT -s 10.0.{{ subnet.stdout }}.0/24 ! -i lo -j ACCEPT"
    backrefs: yes
  
- name: Apply the iptables rules
  ansible.builtin.shell: "iptables-restore < /etc/iptables.rules"

- name: Install prerequisites
  apt:
    name: "{{ item }}"
    state: present
  loop:
    - software-properties-common
    - python3-pip
    - python3-dev
    - build-essential
    - libssl-dev
    - libffi-dev
    - python3-setuptools

- name: Install Python
  apt:
    name: python3.11
    state: present
    update_cache: yes

- name: Download Spark
  get_url:
    url: "https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz"
    dest: "/home/{{ user }}/spark-3.2.0-bin-hadoop3.5.3.tgz"

- name: Create a spark folder
  ansible.builtin.file:
    path: "/home/{{ user }}/spark"
    state: directory

- name: Install Spark
  unarchive:
    src: "/home/{{ user }}/spark-3.2.0-bin-hadoop3.5.3.tgz"
    dest: "/home/{{ user }}/spark/"
    remote_src: yes

- name: Setup bashrc file
  command: |
    echo "export SPARK_HOME=/home/admin/spark/spark-3.5.3-bin-hadoop3" >> /home/{{ user }}/.bashrc
    echo "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin" >> /home/{{ user }}/.bashrc

- name: Replace the default spark-env.sh file
  ansible.builtin.template:
    src: spark-env.sh.master_template
    dest: "/home/{{ user }}/spark/spark-3.5.3-bin-hadoop3/conf/spark-env.sh"
    mode: "0755"

- name: Create the spark-defaults.conf file
  ansible.builtin.template:
    src: spark-defaults.conf.master_template
    dest: "/home/{{ user }}/spark/spark-3.5.3-bin-hadoop3/conf/spark-defaults.conf"
    mode: "0755"

- name: Update the spark-defaults.conf file and spark-env.sh file
  shell: |
    subnet="{{ subnet.stdout }}"
    if [ $subnet = "2" ]; then
      sed -i 's/CHANGE_ME/{{vm1_consumer_private_ip}}/g' /home/{{ user }}/spark/spark-3.5.3-bin-hadoop3/conf/spark-defaults.conf
      sed -i 's/CHANGE_ME/{{vm1_consumer_private_ip}}/g' /home/{{ user }}/spark/spark-3.5.3-bin-hadoop3/conf/spark-env.sh
    else
      sed -i 's/CHANGE_ME/{{vm1_producer_private_ip}}/g' /home/{{ user }}/spark/spark-3.5.3-bin-hadoop3/conf/spark-defaults.conf
      sed -i 's/CHANGE_ME/{{vm1_producer_private_ip}}/g' /home/{{ user }}/spark/spark-3.5.3-bin-hadoop3/conf/spark-env.sh
    fi
  args:
    executable: /bin/bash
  
- name: Remove pre-existing worker file
  command: "rm -rf /home/{{ user }}/spark/spark-3.5.3-bin-hadoop3/conf/workers"

- name: Update the workers file
  shell: |
    subnet="{{ subnet.stdout }}"
    vm2_consumer_private_ip="{{ vm2_consumer_private_ip }}"
    vm3_consumer_private_ip="{{ vm3_consumer_private_ip }}"
    
    vm2_producer_private_ip="{{ vm2_producer_private_ip }}"
    vm3_producer_private_ip="{{ vm3_producer_private_ip }}"

    applyIps() {
      type=$1
      for i in 2 3; do
        vm="vm${i}_${type}_private_ip"
        ipValue=$(eval echo "\${$vm}")
        echo "${ipValue}" >> /home/{{ user }}/spark/spark-3.5.3-bin-hadoop3/conf/workers
      done
    }

    if [ "$subnet" = "2" ]; then 
      applyIps consumer
    else
      applyIps producer
    fi
  args:
    executable: /bin/bash

- name: Check if master
  shell: |
    private_ip=$(hostname -I | awk '{print $1}')

    if [ "$private_ip" = "{{ vm1_consumer_private_ip }}" ] || [ "$private_ip" = "{{ vm1_producer_private_ip }}" ]; then
      echo "master"
    else
      echo "worker"
    fi
  args:
    executable: /bin/bash
  register: is_master

- name: Setup ssh key
  when: is_master.stdout == "master"
  block:
    - name: Generate SSH key
      shell: ssh-keygen -t ed25519 -f /home/{{ user }}/.ssh/id_ed25519 -q -N "" -C "{{ user }}@$(hostname)"
      args:
        creates: "/home/{{ user }}/.ssh/id_ed25519"

    - name: Set proper permissions on private key
      file:
        path: "/home/{{ user }}/.ssh/id_ed25519"
        mode: '0600'
        owner: "{{ user }}"
        group: "{{ user }}"

    - name: Set proper permissions on public key
      file:
        path: "/home/{{ user }}/.ssh/id_ed25519.pub"
        mode: '0644'
        owner: "{{ user }}"
        group: "{{ user }}"

- name: Setup producer
  when: subnet.stdout == "3"
  block:
    - name: Get master producer ssh public key
      shell: cat /home/{{ user }}/.ssh/id_ed25519.pub
      args:
        executable: /bin/bash
      register: master_producer_ssh_pub_key
      when: is_master.stdout == "master" 

    - name: set fact producer
      set_fact:
        master_producer_ssh_pub_key_fact: "{{ hostvars[groups['vm_spark_producer1'][0]].master_producer_ssh_pub_key.stdout }}"

    - name: Setup authorized_keys for workers
      shell: echo "{{ master_producer_ssh_pub_key_fact }}" >> /home/{{ user }}/.ssh/authorized_keys
      args:
        executable: /bin/bash
      when: is_master.stdout == "worker"

- name: Setup consumer
  when: subnet.stdout == "3"
  block:
    - name: Get master consumer ssh public key
      shell: cat /home/{{ user }}/.ssh/id_ed25519.pub
      args:
        executable: /bin/bash
      register: master_consumer_ssh_pub_key
      when: is_master.stdout == "master" 

    - name: set fact consumer
      set_fact:
        master_consumer_ssh_pub_key_fact: "{{ hostvars[groups['vm_spark_consumer1'][0]].master_consumer_ssh_pub_key.stdout }}"

    - name: Setup authorized_keys for workers
      shell: echo "{{ master_consumer_ssh_pub_key_fact }}" >> /home/{{ user }}/.ssh/authorized_keys
      args:
        executable: /bin/bash
      when: is_master.stdout == "worker"
